{"pages":[{"title":"404","text":"","link":"/404.html"},{"title":"BBS","text":"","link":"/bbs/index.html"}],"posts":[{"title":"OpenCV行人计数","text":"在本教程中，您将学习如何使用OpenCV和Python构建“人员计数器”。使用OpenCV，我们可以实时计算“进出”百货商店的人数。 python-opencv行人计数在博客的第一部分，我们将讨论构建行人计数需要的python包。在此，我将简要讨论对象检测和对象跟踪之间的区别，以及我们如何利用两者来创建更准确的人员计数器。 然后，我们将检查项目的目录结构，然后实施整个人员计数项目。 最后，我们将研究将使用OpenCV计数的人数应用于实际视频的结果。 行人计数需要的Python包为了构建行人计数程序，我们需要一些Python库，包括： Numpy OpenCV dlib imutils 理解目标检测与目标跟踪在我们继续本教程的其余部分前，您必须对目标检测和目标跟踪的不同有一个基本的了解。 应用对象检测时，我们要确定对象在图像/帧中的位置。与物体跟踪算法相比，物体检测器通常在计算上也更昂贵，因此更慢。对象检测算法的示例包括Haar级联，HOG +线性SVM，以及基于深度学习的对象检测器，例如Faster R-CNN，YOLO和单发检测器（SSD）。 另一方面，对象跟踪器将接受对象在图像中的位置的输入（x，y）坐标，并且： 为该特定对象分配唯一的ID 跟踪对象在视频流中的移动情况，根据帧的各种属性（渐变，光流等）预测下一帧中的新对象位置 目标检测和目标跟踪的结合高度精确的对象跟踪器会将对象检测和对象跟踪的概念组合为一个算法，通常分为两个阶段： 第一阶段-检测：在检测阶段，我们将运行计算上更昂贵的对象跟踪器，以（1）检测是否有新对象进入我们的视图，以及（2）查看是否可以找到在跟踪阶段“丢失”的对象。对于每个检测到的对象，我们使用新的边界框坐标创建或更新对象跟踪器。由于我们的目标检测器在计算上更加昂贵，因此我们每N帧只运行一次此阶段。 第二阶段-追踪：当我们不处于“检测”阶段时，我们处于“跟踪”阶段。对于我们检测到的每个对象，我们创建一个对象跟踪器以跟踪对象在框架中移动时的情况。我们的对象跟踪器应该比对象检测器更快，更高效。我们将继续跟踪，直到到达第N帧，然后重新运行对象检测器。然后重复整个过程。 这种混合方法的好处是，我们可以应用高精度的对象检测方法，而无需承担太多的计算负担。我们将实现这样的跟踪系统来建立我们的行人计数。 项目结构最重要的两个项目目录： pyimagesearch/：这个模块包含重心跟踪算法。 mobilenet_ssd/：包含Caffe深度学习模型文件。我们将使用MobileNet单发检测器（SSD） 结合目标跟踪算法为了实现我们的行人计数器，我们将同时使用OpenCV和dlib。我们将OpenCV用于标准的计算机视觉/图像处理功能，并将深度学习对象检测器用于人员计数。然后，我们将使用dlib来实现相关性过滤器。我们也可以在这里使用OpenCV。但是，对于该项目，使用dlib对象跟踪实现要容易一些。 下面简述一下质心跟踪算法： 在步骤1，我们接受一组边界框并计算其对应的质心，边界框本身可以通过以下任一方式提供： 对象检测器（例如HOG +线性SVM，Faster R-CNN，SSD等） 或对象跟踪器（例如相关过滤器） 在上图中，您可以看到在此算法的初始迭代中要跟踪两个对象。 在步骤2中，我们计算任何新质心和现有质心之间的欧氏距离： 质心跟踪算法假设，成对的质心之间的最小欧氏距离必须是相同的对象ID。 在上面的示例图像中，我们有两个现有的质心（紫色）和三个新的质心（黄色），这意味着已检测到新对象（因为相对于旧的质心还有一个新的质心）。 然后，箭头代表计算所有紫色质心和所有黄色质心之间的欧几里得距离。 有了欧氏距离后，我们尝试在步骤3中关联对象ID： 在图4中，您可以看到我们的质心跟踪器选择了关联以最小化其各自的欧几里得距离的质心。 但是左下角的点呢？ 它没有任何关联-我们该怎么办？ 要回答这个问题，我们需要执行步骤4，注册新对象： 注册只是意味着我们可以通过以下方式将新对象添加到跟踪对象列表中： 为他分配一个新的对象ID 存储新对象边界框坐标的质心 如果物体丢失或离开视野，我们可以简单地注销该物体（步骤5）。 确切地说，当对象“丢失”或“不再可见”时，您如何处理取决于您的确切应用，但是对于我们的人员计数器，当人员ID在连续40帧中无法与任何现有人员对象匹配时，我们将注销人员ID 。 同样，这只是质心跟踪算法的简要概述。 创建一个可跟踪对象为了跟踪和计数视频流中的对象，我们需要一种简单的方法来存储有关对象本身的信息，包括： 一个对象ID 它是先前的质心（因此我们可以轻松计算对象移动的方向） 对象是否已经被计数 为了实现所有这些目标，我们能定义一个TrackableObject实例，打开trackableobject.py 并插入以下代码： 12345678910class TrackableObject: def __init__(self, objectID, centroid): # store the object ID, then initialize a list of centroids # using the current centroid self.objectID = objectID self.centroids = [centroid] # initialize a boolean used to indicate if the object has # already been counted or not self.counted = False 构造函数TrackableObject接收一个objectID和centroid并存储他们。质心变量是一个list因为它需要包含一个对象质心的历史位置。 这个构造函数也吧counter初始化为False，指示尚未计算对象。 用OpenCV+Python实现我们的行人计数有了我们所有支持的Python帮助工具和类之后，我们现在就可以构建我们的OpenCV人员计数器了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294# USAGE# To read and write back out to video:# python people_counter.py --prototxt mobilenet_ssd/MobileNetSSD_deploy.prototxt \\# --model mobilenet_ssd/MobileNetSSD_deploy.caffemodel --input videos/example_01.mp4 \\# --output output/output_01.avi## To read from webcam and write back out to disk:# python people_counter.py --prototxt mobilenet_ssd/MobileNetSSD_deploy.prototxt \\# --model mobilenet_ssd/MobileNetSSD_deploy.caffemodel \\# --output output/webcam_output.avi# import the necessary packagesfrom pyimagesearch.centroidtracker import CentroidTrackerfrom pyimagesearch.trackableobject import TrackableObjectfrom imutils.video import VideoStreamfrom imutils.video import FPSimport numpy as npimport argparseimport imutilsimport timeimport dlibimport cv2# construct the argument parse and parse the argumentsap = argparse.ArgumentParser()ap.add_argument(\"-p\", \"--prototxt\", required=True, help=\"path to Caffe 'deploy' prototxt file\")ap.add_argument(\"-m\", \"--model\", required=True, help=\"path to Caffe pre-trained model\")ap.add_argument(\"-i\", \"--input\", type=str, help=\"path to optional input video file\")ap.add_argument(\"-o\", \"--output\", type=str, help=\"path to optional output video file\")ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.4, help=\"minimum probability to filter weak detections\")ap.add_argument(\"-s\", \"--skip-frames\", type=int, default=30, help=\"# of skip frames between detections\")args = vars(ap.parse_args())# initialize the list of class labels MobileNet SSD was trained to# detectCLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]# load our serialized model from diskprint(\"[INFO] loading model...\")net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])# if a video path was not supplied, grab a reference to the webcamif not args.get(\"input\", False): print(\"[INFO] starting video stream...\") vs = VideoStream(src=0).start() time.sleep(2.0)# otherwise, grab a reference to the video fileelse: print(\"[INFO] opening video file...\") vs = cv2.VideoCapture(args[\"input\"])# initialize the video writer (we'll instantiate later if need be)writer = None# initialize the frame dimensions (we'll set them as soon as we read# the first frame from the video)W = NoneH = None# instantiate our centroid tracker, then initialize a list to store# each of our dlib correlation trackers, followed by a dictionary to# map each unique object ID to a TrackableObjectct = CentroidTracker(maxDisappeared=40, maxDistance=50)trackers = []trackableObjects = {}# initialize the total number of frames processed thus far, along# with the total number of objects that have moved either up or downtotalFrames = 0totalDown = 0totalUp = 0# start the frames per second throughput estimatorfps = FPS().start()# loop over frames from the video streamwhile True: # grab the next frame and handle if we are reading from either # VideoCapture or VideoStream frame = vs.read() frame = frame[1] if args.get(\"input\", False) else frame # if we are viewing a video and we did not grab a frame then we # have reached the end of the video if args[\"input\"] is not None and frame is None: break # resize the frame to have a maximum width of 500 pixels (the # less data we have, the faster we can process it), then convert # the frame from BGR to RGB for dlib frame = imutils.resize(frame, width=500) rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # if the frame dimensions are empty, set them if W is None or H is None: (H, W) = frame.shape[:2] # if we are supposed to be writing a video to disk, initialize # the writer if args[\"output\"] is not None and writer is None: fourcc = cv2.VideoWriter_fourcc(*\"MJPG\") writer = cv2.VideoWriter(args[\"output\"], fourcc, 30, (W, H), True) # initialize the current status along with our list of bounding # box rectangles returned by either (1) our object detector or # (2) the correlation trackers status = \"Waiting\" rects = [] # check to see if we should run a more computationally expensive # object detection method to aid our tracker if totalFrames % args[\"skip_frames\"] == 0: # set the status and initialize our new set of object trackers status = \"Detecting\" trackers = [] # convert the frame to a blob and pass the blob through the # network and obtain the detections blob = cv2.dnn.blobFromImage(frame, 0.007843, (W, H), 127.5) net.setInput(blob) detections = net.forward() # loop over the detections for i in np.arange(0, detections.shape[2]): # extract the confidence (i.e., probability) associated # with the prediction confidence = detections[0, 0, i, 2] # filter out weak detections by requiring a minimum # confidence if confidence &gt; args[\"confidence\"]: # extract the index of the class label from the # detections list idx = int(detections[0, 0, i, 1]) # if the class label is not a person, ignore it if CLASSES[idx] != \"person\": continue # compute the (x, y)-coordinates of the bounding box # for the object box = detections[0, 0, i, 3:7] * np.array([W, H, W, H]) (startX, startY, endX, endY) = box.astype(\"int\") # construct a dlib rectangle object from the bounding # box coordinates and then start the dlib correlation # tracker tracker = dlib.correlation_tracker() rect = dlib.rectangle(startX, startY, endX, endY) tracker.start_track(rgb, rect) # add the tracker to our list of trackers so we can # utilize it during skip frames trackers.append(tracker) # otherwise, we should utilize our object *trackers* rather than # object *detectors* to obtain a higher frame processing throughput else: # loop over the trackers for tracker in trackers: # set the status of our system to be 'tracking' rather # than 'waiting' or 'detecting' status = \"Tracking\" # update the tracker and grab the updated position tracker.update(rgb) pos = tracker.get_position() # unpack the position object startX = int(pos.left()) startY = int(pos.top()) endX = int(pos.right()) endY = int(pos.bottom()) # add the bounding box coordinates to the rectangles list rects.append((startX, startY, endX, endY)) # draw a horizontal line in the center of the frame -- once an # object crosses this line we will determine whether they were # moving 'up' or 'down' cv2.line(frame, (0, H // 2), (W, H // 2), (0, 255, 255), 2) # use the centroid tracker to associate the (1) old object # centroids with (2) the newly computed object centroids objects = ct.update(rects) # loop over the tracked objects for (objectID, centroid) in objects.items(): # check to see if a trackable object exists for the current # object ID to = trackableObjects.get(objectID, None) # if there is no existing trackable object, create one if to is None: to = TrackableObject(objectID, centroid) # otherwise, there is a trackable object so we can utilize it # to determine direction else: # the difference between the y-coordinate of the *current* # centroid and the mean of *previous* centroids will tell # us in which direction the object is moving (negative for # 'up' and positive for 'down') y = [c[1] for c in to.centroids] direction = centroid[1] - np.mean(y) to.centroids.append(centroid) # check to see if the object has been counted or not if not to.counted: # if the direction is negative (indicating the object # is moving up) AND the centroid is above the center # line, count the object if direction &lt; 0 and abs(centroid[1] - H // 2) &lt; 10: totalUp += 1 to.counted = True # if the direction is positive (indicating the object # is moving down) AND the centroid is below the # center line, count the object elif direction &gt; 0 and abs(centroid[1] - H // 2) &lt; 10: totalDown += 1 to.counted = True # store the trackable object in our dictionary trackableObjects[objectID] = to # draw both the ID of the object and the centroid of the # object on the output frame text = \"ID {}\".format(objectID) cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2) cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1) # construct a tuple of information we will be displaying on the # frame info = [ (\"Up\", totalUp), (\"Down\", totalDown), (\"Status\", status), ] # loop over the info tuples and draw them on our frame for (i, (k, v)) in enumerate(info): text = \"{}: {}\".format(k, v) cv2.putText(frame, text, (10, H - ((i * 20) + 20)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2) # check to see if we should write the frame to disk if writer is not None: writer.write(frame) # show the output frame cv2.imshow(\"Frame\", frame) key = cv2.waitKey(1) &amp; 0xFF # if the `q` key was pressed, break from the loop if key == ord(\"q\"): break # increment the total number of frames processed thus far and # then update the FPS counter totalFrames += 1 fps.update()# stop the timer and display FPS informationfps.stop()print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))# check to see if we need to release the video writer pointerif writer is not None: writer.release()# if we are not using a video file, stop the camera video streamif not args.get(\"input\", False): vs.stop()# otherwise, release the video file pointerelse: vs.release()# close any open windowscv2.destroyAllWindows()","link":"/2019/12/03/opencv行人计数/"},{"title":"在开始一个成功的机器学习项目之前要问的11个问题","text":"机器学习算法正在改变几乎每个产业。他们正提高生产力，促进销售并帮我们做出更明智的决定。很多组织也早已在利用机器学习的功能，或者放在其规划中作为一个值得追求的机会。 问题是ML很复杂，当开始一个ML项目的时候你很可能会问自己需要考虑什么。在你启动一个ML计划前，你应该花一些时间来计划。这将有助于最大程度减少潜在风险并最大化积极成果。 这十一个问题会考虑到你的策略，文化，资源和数据。这些问题将帮助你为ML项目构建战略路线图，并使实施更进一步。 STRATEGY1. 你组织的业务目标是什么？即使将ML作为实验开始，也应该始终有一个最终的业务目标，该目标实际上很重要，并且可能会受到ML的影响。机器学习项目的成功标准应该不是一定的准确度，而是业务指标。 例如，假设您正在零售领域，现在的主要重点是降低成本。一个特定的业务目标可能是在下半年末将仓库成本降低10％。实现此目标的一种方法可能是通过从ML算法获得的优化。 2. ML是应该降低成本还是增加收入？成功的机器学习项目可以降低成本或增加收入（或两者都有），您应该定义目标是什么。只有这样，ML才能对您的组织及其成长产生重大影响。 借助机器学习降低成本的示例包括虚拟助手和聊天机器人，预测性维护和库存优化。相反，实施了诸如价格优化，通过视频分析或推荐系统进行行为客户跟踪之类的解决方案来增加收入。 3. 衡量ML计划成功的清晰而现实的方法是什么?每个ML项目都是不同的，您需要定义一个成功指标，该指标对您的特定项目有意义，并且可以衡量。设置该指标后，您应该确保它被业务人员和数据科学家所接受。 机器学习计划的成功指标可能是工作时间，客户查询响应时间或准确性百分比。 4. 你的组织如何处理风险？与所有创新一样，由于多种因素，机器学习计划的结果可能不会如预期的那样出现。也许您发现需要10倍的数据才能做有意义的事情，或者数据根本不够好。这就是为什么您应该考虑ML计划失败的风险，并能够在项目未达到您的期望的情况下转到另一个项目。 CULTURE/RESOURCES5. 如何获得合适的人才？由于对ML的专业需求很高，因此在市场上很难获得。当您无法找到合适的人（或没有足够的人）来从事该项目时，完全或部分外包ML项目是一个不错的选择，尤其是在时机很重要且您希望早日实施解决方案时。 6. 你有没有对ML是什么有更清晰的理解？了解不同的用例，需要什么输入ML算法以及什么时候不能应用它们，可以帮助您在项目中做出正确的决定。您应该从概念上理解ML给图片带来的好处，例如，通过阅读机器学习博客和案例研究，参加会议或与专家交谈。 7. 是否保证信息的获取？当启动ML计划时，数据科学家需要轻松访问信息。他们将需要与关键人员（可能来自不同部门，例如IT部门）合作。那些人需要用某种商业知识来支持该项目，而内部官僚主义将是主要的制约因素。理想情况下，机器学习和数据科学应及时遍及整个组织。 “数据科学是新的统计方法”。 8. 是否将该项目计划成中期项目？机器学习计划的结果需要时间，您将无法在一夜之间证明自己的成功。这需要你的组织都把机器学习的主动性作为中期项目。在软件开发世界中，时间估计一直是一个挑战。在机器学习开发的情况下，这一挑战更加艰巨，因为流程本身具有更多的不确定性。有时，您可能会停滞不前，简单的突破就可以彻底扭转局面。也就是说，您应该注意，可能需要花费几个月的时间才能获得理想的结果。 DATA9. 你的组织收集了正确的数据吗？机器学习算法不是魔法。他们需要数据才能工作，并且只能与您输入的数据一样好。根据您的目标，有不同的方法和资源来收集正确的数据。无论如何，您拥有的输入数据越多，机器学习模型表现良好的机会就越大。如果您对数据的数量和质量有疑问，可以请数据科学家帮助您评估数据集，并在必要时找到获取第三方数据的最佳方法。 10. 你的组织是否以正确的格式收集数据？除了拥有正确数量和类型的数据外，您还应确保以正确的格式收集数据。想象一下，您已经为智能手机拍摄了数千张完美的照片（高分辨率和白色背景），以训练计算机视觉模型来检测图像中的智能手机。然后，您发现它不起作用，因为实际用例是检测到在各种照明/对比度/背景下拿着智能手机的人，而不是自己。您过去的数据收集工作几乎毫无价值，您需要重新开始。另外，您应该了解所收集的数据中是否存在偏差，因为ML算法将学习该偏差。 11. 是否考虑过人工标注数据？根据项目，机器学习解决方案可以基于监督算法。这些算法要求对收集的数据进行标记，即。人们需要为我们收集的每个示例指定预期的结果，以便算法可以从这些见解中学习。确保在项目预算中包括构建此类数据集的人员成本。","link":"/2019/11/01/开始一个机器学习项目需要知道的/"},{"title":"厦门","text":"个人觉得厦门没什么好玩的，被炒起来的网红城市。厦门的慢体现在生活中吧，如果去玩还是会差些。最毒的就是每天晚上都要去曾厝垵吃东西（芒果），吃到撑。","link":"/2019/10/12/厦门/"},{"title":"图像识别工程步骤","text":"数据准备 拍摄照片或下载数据集 图像标注 图像数据处理（图像合成、图像裁剪、图像改变大小） 检查标注文件的正确性 划分训练集、验证集 搭建神经网络 模型训练（读取数据的多线程生成器、多GPU并行训练、保存模型） 模型评估（加载模型、使用业务对应的评估指标在验证集上计算模型效果） 模型测试（使用实际场景中的图像数据测试模型效果，单张图片检查、实时视频检测） 模型部署（开发web服务端接口、前端界面） 闭环优化 根据业务使用场景在第2步优化 根据模型在实际场景的使用效果在3-6步间循环迭代。如果新加入的数据提高了模型在验证集上的效果，则替换模型部署使用的权重文件。 如果替换后的新权重文件在实际场景中的使用效果不好，则使用版本控制工具回退到上一个版本的权重文件。","link":"/2019/09/30/图像识别工程大概步骤/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/08/30/hello-world/"},{"title":"hello","text":"aa今天是0831","link":"/2019/08/28/hello/"},{"title":"My New Post","text":"这是第二个博客 2019年8月29","link":"/2019/08/28/My-New-Post/"},{"title":"A SImple Post'","text":"Here is simaple markdown","link":"/2019/08/28/Post/"},{"title":"test_my_site","text":"","link":"/2019/08/28/test-my-site/"}],"tags":[{"name":"Travel","slug":"Travel","link":"/tags/Travel/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"}],"categories":[]}