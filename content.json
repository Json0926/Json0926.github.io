{"pages":[{"title":"404","text":"","link":"/404.html"},{"title":"BBS","text":"","link":"/bbs/index.html"}],"posts":[{"title":"OpenCV行人计数","text":"在本教程中，您将学习如何使用OpenCV和Python构建“人员计数器”。使用OpenCV，我们可以实时计算“进出”百货商店的人数。 python-opencv行人计数在博客的第一部分，我们将讨论构建行人计数需要的python包。在此，我将简要讨论对象检测和对象跟踪之间的区别，以及我们如何利用两者来创建更准确的人员计数器。 然后，我们将检查项目的目录结构，然后实施整个人员计数项目。 最后，我们将研究将使用OpenCV计数的人数应用于实际视频的结果。 行人计数需要的Python包为了构建行人计数程序，我们需要一些Python库，包括： Numpy OpenCV dlib imutils 理解目标检测与目标跟踪在我们继续本教程的其余部分前，您必须对目标检测和目标跟踪的不同有一个基本的了解。 应用对象检测时，我们要确定对象在图像/帧中的位置。与物体跟踪算法相比，物体检测器通常在计算上也更昂贵，因此更慢。对象检测算法的示例包括Haar级联，HOG +线性SVM，以及基于深度学习的对象检测器，例如Faster R-CNN，YOLO和单发检测器（SSD）。 另一方面，对象跟踪器将接受对象在图像中的位置的输入（x，y）坐标，并且： 为该特定对象分配唯一的ID 跟踪对象在视频流中的移动情况，根据帧的各种属性（渐变，光流等）预测下一帧中的新对象位置 目标检测和目标跟踪的结合高度精确的对象跟踪器会将对象检测和对象跟踪的概念组合为一个算法，通常分为两个阶段： 第一阶段-检测：在检测阶段，我们将运行计算上更昂贵的对象跟踪器，以（1）检测是否有新对象进入我们的视图，以及（2）查看是否可以找到在跟踪阶段“丢失”的对象。对于每个检测到的对象，我们使用新的边界框坐标创建或更新对象跟踪器。由于我们的目标检测器在计算上更加昂贵，因此我们每N帧只运行一次此阶段。 第二阶段-追踪：当我们不处于“检测”阶段时，我们处于“跟踪”阶段。对于我们检测到的每个对象，我们创建一个对象跟踪器以跟踪对象在框架中移动时的情况。我们的对象跟踪器应该比对象检测器更快，更高效。我们将继续跟踪，直到到达第N帧，然后重新运行对象检测器。然后重复整个过程。 这种混合方法的好处是，我们可以应用高精度的对象检测方法，而无需承担太多的计算负担。我们将实现这样的跟踪系统来建立我们的行人计数。 项目结构最重要的两个项目目录： pyimagesearch/：这个模块包含重心跟踪算法。 mobilenet_ssd/：包含Caffe深度学习模型文件。我们将使用MobileNet单发检测器（SSD） 结合目标跟踪算法为了实现我们的行人计数器，我们将同时使用OpenCV和dlib。我们将OpenCV用于标准的计算机视觉/图像处理功能，并将深度学习对象检测器用于人员计数。然后，我们将使用dlib来实现相关性过滤器。我们也可以在这里使用OpenCV。但是，对于该项目，使用dlib对象跟踪实现要容易一些。 下面简述一下质心跟踪算法： 在步骤1，我们接受一组边界框并计算其对应的质心，边界框本身可以通过以下任一方式提供： 对象检测器（例如HOG +线性SVM，Faster R-CNN，SSD等） 或对象跟踪器（例如相关过滤器） 在上图中，您可以看到在此算法的初始迭代中要跟踪两个对象。 在步骤2中，我们计算任何新质心和现有质心之间的欧氏距离： 质心跟踪算法假设，成对的质心之间的最小欧氏距离必须是相同的对象ID。 在上面的示例图像中，我们有两个现有的质心（紫色）和三个新的质心（黄色），这意味着已检测到新对象（因为相对于旧的质心还有一个新的质心）。 然后，箭头代表计算所有紫色质心和所有黄色质心之间的欧几里得距离。 有了欧氏距离后，我们尝试在步骤3中关联对象ID： 在图4中，您可以看到我们的质心跟踪器选择了关联以最小化其各自的欧几里得距离的质心。 但是左下角的点呢？ 它没有任何关联-我们该怎么办？ 要回答这个问题，我们需要执行步骤4，注册新对象： 注册只是意味着我们可以通过以下方式将新对象添加到跟踪对象列表中： 为他分配一个新的对象ID 存储新对象边界框坐标的质心 如果物体丢失或离开视野，我们可以简单地注销该物体（步骤5）。 确切地说，当对象“丢失”或“不再可见”时，您如何处理取决于您的确切应用，但是对于我们的人员计数器，当人员ID在连续40帧中无法与任何现有人员对象匹配时，我们将注销人员ID 。 同样，这只是质心跟踪算法的简要概述。 创建一个可跟踪对象为了跟踪和计数视频流中的对象，我们需要一种简单的方法来存储有关对象本身的信息，包括： 一个对象ID 它是先前的质心（因此我们可以轻松计算对象移动的方向） 对象是否已经被计数 为了实现所有这些目标，我们能定义一个TrackableObject实例，打开trackableobject.py 并插入以下代码： 12345678910class TrackableObject: def __init__(self, objectID, centroid): # store the object ID, then initialize a list of centroids # using the current centroid self.objectID = objectID self.centroids = [centroid] # initialize a boolean used to indicate if the object has # already been counted or not self.counted = False 构造函数TrackableObject接收一个objectID和centroid并存储他们。质心变量是一个list因为它需要包含一个对象质心的历史位置。 这个构造函数也吧counter初始化为False，指示尚未计算对象。 用OpenCV+Python实现我们的行人计数有了我们所有支持的Python帮助工具和类之后，我们现在就可以构建我们的OpenCV人员计数器了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294# USAGE# To read and write back out to video:# python people_counter.py --prototxt mobilenet_ssd/MobileNetSSD_deploy.prototxt \\# --model mobilenet_ssd/MobileNetSSD_deploy.caffemodel --input videos/example_01.mp4 \\# --output output/output_01.avi## To read from webcam and write back out to disk:# python people_counter.py --prototxt mobilenet_ssd/MobileNetSSD_deploy.prototxt \\# --model mobilenet_ssd/MobileNetSSD_deploy.caffemodel \\# --output output/webcam_output.avi# import the necessary packagesfrom pyimagesearch.centroidtracker import CentroidTrackerfrom pyimagesearch.trackableobject import TrackableObjectfrom imutils.video import VideoStreamfrom imutils.video import FPSimport numpy as npimport argparseimport imutilsimport timeimport dlibimport cv2# construct the argument parse and parse the argumentsap = argparse.ArgumentParser()ap.add_argument(\"-p\", \"--prototxt\", required=True, help=\"path to Caffe 'deploy' prototxt file\")ap.add_argument(\"-m\", \"--model\", required=True, help=\"path to Caffe pre-trained model\")ap.add_argument(\"-i\", \"--input\", type=str, help=\"path to optional input video file\")ap.add_argument(\"-o\", \"--output\", type=str, help=\"path to optional output video file\")ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.4, help=\"minimum probability to filter weak detections\")ap.add_argument(\"-s\", \"--skip-frames\", type=int, default=30, help=\"# of skip frames between detections\")args = vars(ap.parse_args())# initialize the list of class labels MobileNet SSD was trained to# detectCLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]# load our serialized model from diskprint(\"[INFO] loading model...\")net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])# if a video path was not supplied, grab a reference to the webcamif not args.get(\"input\", False): print(\"[INFO] starting video stream...\") vs = VideoStream(src=0).start() time.sleep(2.0)# otherwise, grab a reference to the video fileelse: print(\"[INFO] opening video file...\") vs = cv2.VideoCapture(args[\"input\"])# initialize the video writer (we'll instantiate later if need be)writer = None# initialize the frame dimensions (we'll set them as soon as we read# the first frame from the video)W = NoneH = None# instantiate our centroid tracker, then initialize a list to store# each of our dlib correlation trackers, followed by a dictionary to# map each unique object ID to a TrackableObjectct = CentroidTracker(maxDisappeared=40, maxDistance=50)trackers = []trackableObjects = {}# initialize the total number of frames processed thus far, along# with the total number of objects that have moved either up or downtotalFrames = 0totalDown = 0totalUp = 0# start the frames per second throughput estimatorfps = FPS().start()# loop over frames from the video streamwhile True: # grab the next frame and handle if we are reading from either # VideoCapture or VideoStream frame = vs.read() frame = frame[1] if args.get(\"input\", False) else frame # if we are viewing a video and we did not grab a frame then we # have reached the end of the video if args[\"input\"] is not None and frame is None: break # resize the frame to have a maximum width of 500 pixels (the # less data we have, the faster we can process it), then convert # the frame from BGR to RGB for dlib frame = imutils.resize(frame, width=500) rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # if the frame dimensions are empty, set them if W is None or H is None: (H, W) = frame.shape[:2] # if we are supposed to be writing a video to disk, initialize # the writer if args[\"output\"] is not None and writer is None: fourcc = cv2.VideoWriter_fourcc(*\"MJPG\") writer = cv2.VideoWriter(args[\"output\"], fourcc, 30, (W, H), True) # initialize the current status along with our list of bounding # box rectangles returned by either (1) our object detector or # (2) the correlation trackers status = \"Waiting\" rects = [] # check to see if we should run a more computationally expensive # object detection method to aid our tracker if totalFrames % args[\"skip_frames\"] == 0: # set the status and initialize our new set of object trackers status = \"Detecting\" trackers = [] # convert the frame to a blob and pass the blob through the # network and obtain the detections blob = cv2.dnn.blobFromImage(frame, 0.007843, (W, H), 127.5) net.setInput(blob) detections = net.forward() # loop over the detections for i in np.arange(0, detections.shape[2]): # extract the confidence (i.e., probability) associated # with the prediction confidence = detections[0, 0, i, 2] # filter out weak detections by requiring a minimum # confidence if confidence &gt; args[\"confidence\"]: # extract the index of the class label from the # detections list idx = int(detections[0, 0, i, 1]) # if the class label is not a person, ignore it if CLASSES[idx] != \"person\": continue # compute the (x, y)-coordinates of the bounding box # for the object box = detections[0, 0, i, 3:7] * np.array([W, H, W, H]) (startX, startY, endX, endY) = box.astype(\"int\") # construct a dlib rectangle object from the bounding # box coordinates and then start the dlib correlation # tracker tracker = dlib.correlation_tracker() rect = dlib.rectangle(startX, startY, endX, endY) tracker.start_track(rgb, rect) # add the tracker to our list of trackers so we can # utilize it during skip frames trackers.append(tracker) # otherwise, we should utilize our object *trackers* rather than # object *detectors* to obtain a higher frame processing throughput else: # loop over the trackers for tracker in trackers: # set the status of our system to be 'tracking' rather # than 'waiting' or 'detecting' status = \"Tracking\" # update the tracker and grab the updated position tracker.update(rgb) pos = tracker.get_position() # unpack the position object startX = int(pos.left()) startY = int(pos.top()) endX = int(pos.right()) endY = int(pos.bottom()) # add the bounding box coordinates to the rectangles list rects.append((startX, startY, endX, endY)) # draw a horizontal line in the center of the frame -- once an # object crosses this line we will determine whether they were # moving 'up' or 'down' cv2.line(frame, (0, H // 2), (W, H // 2), (0, 255, 255), 2) # use the centroid tracker to associate the (1) old object # centroids with (2) the newly computed object centroids objects = ct.update(rects) # loop over the tracked objects for (objectID, centroid) in objects.items(): # check to see if a trackable object exists for the current # object ID to = trackableObjects.get(objectID, None) # if there is no existing trackable object, create one if to is None: to = TrackableObject(objectID, centroid) # otherwise, there is a trackable object so we can utilize it # to determine direction else: # the difference between the y-coordinate of the *current* # centroid and the mean of *previous* centroids will tell # us in which direction the object is moving (negative for # 'up' and positive for 'down') y = [c[1] for c in to.centroids] direction = centroid[1] - np.mean(y) to.centroids.append(centroid) # check to see if the object has been counted or not if not to.counted: # if the direction is negative (indicating the object # is moving up) AND the centroid is above the center # line, count the object if direction &lt; 0 and abs(centroid[1] - H // 2) &lt; 10: totalUp += 1 to.counted = True # if the direction is positive (indicating the object # is moving down) AND the centroid is below the # center line, count the object elif direction &gt; 0 and abs(centroid[1] - H // 2) &lt; 10: totalDown += 1 to.counted = True # store the trackable object in our dictionary trackableObjects[objectID] = to # draw both the ID of the object and the centroid of the # object on the output frame text = \"ID {}\".format(objectID) cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2) cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1) # construct a tuple of information we will be displaying on the # frame info = [ (\"Up\", totalUp), (\"Down\", totalDown), (\"Status\", status), ] # loop over the info tuples and draw them on our frame for (i, (k, v)) in enumerate(info): text = \"{}: {}\".format(k, v) cv2.putText(frame, text, (10, H - ((i * 20) + 20)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2) # check to see if we should write the frame to disk if writer is not None: writer.write(frame) # show the output frame cv2.imshow(\"Frame\", frame) key = cv2.waitKey(1) &amp; 0xFF # if the `q` key was pressed, break from the loop if key == ord(\"q\"): break # increment the total number of frames processed thus far and # then update the FPS counter totalFrames += 1 fps.update()# stop the timer and display FPS informationfps.stop()print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))# check to see if we need to release the video writer pointerif writer is not None: writer.release()# if we are not using a video file, stop the camera video streamif not args.get(\"input\", False): vs.stop()# otherwise, release the video file pointerelse: vs.release()# close any open windowscv2.destroyAllWindows()","link":"/2019/12/03/opencv行人计数/"},{"title":"OpenCV实现目标跟踪","text":"本节内容是学习实现如何通过OpenCV实现质心跟踪，一个易于理解且高效的跟踪算法。 目标跟踪的过程： 进行一组初始的对象检测（例如边界框坐标的输入集） 为每一个初始检测对象创建一个唯一ID 然后跟踪每个对象在视频中的帧中移动时的情况，并保持唯一ID的分配 此外，目标跟踪允许我们将唯一的ID应用于每个被跟踪的对象，从而使我们能够对视频中的唯一对象进行计数。目标跟踪对于建立人员计数器至关重要。 理想的目标跟踪算法将会： 仅仅需要请求一次目标检测阶段 将非常快-比运行实际的对象检测器本身快得多 能够处理当被跟踪对象“消失”或移动到视频帧的边界之外 抗遮挡能力强 能够拾取帧之间丢失的目标 对于任何计算机视觉和图像处理算法来说这都是一个很高的要求，并且我们可以发挥多种技巧来帮助提高我们的目标检测器。 但是在我们构建一个鲁棒的方法之前，我们首先需要研究目标跟踪的基础。 目标跟踪算法被叫做质心跟踪算法是因为它依赖于以下两者的欧式距离： 已经存在的目标质心 视频后续帧中的新目标质心 我将实现一个包含质心追踪算法的Python类，并且创建一个脚本去导入视频并运行目标追踪器。 最后，将运行目标检测器并检查结果，同时注意算法的优缺点。 质心跟踪算法质心跟踪算法是一个多步骤过程。下面回顾每一个跟踪步骤。 步骤1: 接收边界框坐标并计算质心 质心跟踪算法假设我们正在传递每个单帧中每个检测到的对象的一组边界框（x，y）坐标。 这些边界框可以由您想要的任何类型的对象检测器生成（颜色阈值+轮廓提取，Haar级联，HOG +线性SVM，SSD，Faster R-CNN等），前提是要针对帧中的每个帧进行计算该视频。 有了边界框坐标后，我们必须计算“质心”或更简单地计算边界框的中心坐标(x, y)。上图演示了接收一组边界框坐标并计算质心。 由于这些是提供给我们算法的边界框的第一组初始集合，因此我们将为其分配唯一的ID。 步骤2 计算新边界框和存在目标的欧式距离 对于视频流中的每个后续帧，我们应用计算对象质心的步骤1。但是，我们首先需要确定是否可以将新的对象质心（黄色）与旧的对象质心（紫色）相关联，而不是为每个检测到的对象分配新的唯一ID（这会破坏对象跟踪的目的）。为了完成此过程，我们计算了每对现有对象质心和输入对象质心之间的欧式距离（用绿色箭头突出显示）。 从上图中你可以看到我们在图像中检测到三个物体。靠近的两对是两个存在的目标。 然后，我们计算每对原始质心（黄色）和新质心（紫色）之间的欧式距离。但是，我们如何使用这些点之间的欧式距离来实际匹配它们并将它们关联起来呢？ 步骤3：更新存在目标的(x, y)坐标 质心跟踪算法的主要假设是，给定的对象可能会在后续帧之间移动，但是帧的质心之间的距离将小于对象之间的所有其他距离。 因此，如果我们选择将质心与后续帧之间的最小距离相关联，则可以构建对象跟踪器。 上图中，你可以看到质心追踪算法如何选择将最小化欧式距离的质心关联起来。 但是左下角没有任何可以关联的点需要怎么办？ 步骤4：注册新的目标 如果输入检测的数量多于被跟踪的现有对象，需要注册一个新目标。注册意味着需要添加一个新的对象到追踪列表中，通过： 给它分配一个新的对象ID 存储该对象的边界框坐标的质心 现在回到步骤2对视频流中的每一帧重复步骤。 上图演示了使用最小欧式距离来关联现有对象ID，然后注册一个新对象的过程。 步骤5：注销旧的目标任何合理的对象跟踪算法都必须能够处理对象丢失，消失或离开视野时的情况。实际情况下，如何处理这些情况实际上取决于对象跟踪器的部署位置，但是对于此实现，我们将在旧对象无法与任何现有对象匹配（总共N个后续帧）时注销旧对象。 目标跟踪项目结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163# import the necessary packagesfrom scipy.spatial import distance as distfrom collections import OrderedDictimport numpy as npclass CentroidTracker: def __init__(self, maxDisappeared=50, maxDistance=50): # initialize the next unique object ID along with two ordered # dictionaries used to keep track of mapping a given object # ID to its centroid and number of consecutive frames it has # been marked as \"disappeared\", respectively self.nextObjectID = 0 self.objects = OrderedDict() self.disappeared = OrderedDict() # store the number of maximum consecutive frames a given # object is allowed to be marked as \"disappeared\" until we # need to deregister the object from tracking self.maxDisappeared = maxDisappeared # store the maximum distance between centroids to associate # an object -- if the distance is larger than this maximum # distance we'll start to mark the object as \"disappeared\" self.maxDistance = maxDistance def register(self, centroid): # when registering an object we use the next available object # ID to store the centroid self.objects[self.nextObjectID] = centroid self.disappeared[self.nextObjectID] = 0 self.nextObjectID += 1 def deregister(self, objectID): # to deregister an object ID we delete the object ID from # both of our respective dictionaries del self.objects[objectID] del self.disappeared[objectID] def update(self, rects): # check to see if the list of input bounding box rectangles # is empty if len(rects) == 0: # loop over any existing tracked objects and mark them # as disappeared for objectID in list(self.disappeared.keys()): self.disappeared[objectID] += 1 # if we have reached a maximum number of consecutive # frames where a given object has been marked as # missing, deregister it if self.disappeared[objectID] &gt; self.maxDisappeared: self.deregister(objectID) # return early as there are no centroids or tracking info # to update return self.objects # initialize an array of input centroids for the current frame inputCentroids = np.zeros((len(rects), 2), dtype=\"int\") # loop over the bounding box rectangles for (i, (startX, startY, endX, endY)) in enumerate(rects): # use the bounding box coordinates to derive the centroid cX = int((startX + endX) / 2.0) cY = int((startY + endY) / 2.0) inputCentroids[i] = (cX, cY) # if we are currently not tracking any objects take the input # centroids and register each of them if len(self.objects) == 0: for i in range(0, len(inputCentroids)): self.register(inputCentroids[i]) # otherwise, are are currently tracking objects so we need to # try to match the input centroids to existing object # centroids else: # grab the set of object IDs and corresponding centroids objectIDs = list(self.objects.keys()) objectCentroids = list(self.objects.values()) # compute the distance between each pair of object # centroids and input centroids, respectively -- our # goal will be to match an input centroid to an existing # object centroid D = dist.cdist(np.array(objectCentroids), inputCentroids) # in order to perform this matching we must (1) find the # smallest value in each row and then (2) sort the row # indexes based on their minimum values so that the row # with the smallest value as at the *front* of the index # list rows = D.min(axis=1).argsort() # next, we perform a similar process on the columns by # finding the smallest value in each column and then # sorting using the previously computed row index list cols = D.argmin(axis=1)[rows] # in order to determine if we need to update, register, # or deregister an object we need to keep track of which # of the rows and column indexes we have already examined usedRows = set() usedCols = set() # loop over the combination of the (row, column) index # tuples for (row, col) in zip(rows, cols): # if we have already examined either the row or # column value before, ignore it if row in usedRows or col in usedCols: continue # if the distance between centroids is greater than # the maximum distance, do not associate the two # centroids to the same object if D[row, col] &gt; self.maxDistance: continue # otherwise, grab the object ID for the current row, # set its new centroid, and reset the disappeared # counter objectID = objectIDs[row] self.objects[objectID] = inputCentroids[col] self.disappeared[objectID] = 0 # indicate that we have examined each of the row and # column indexes, respectively usedRows.add(row) usedCols.add(col) # compute both the row and column index we have NOT yet # examined unusedRows = set(range(0, D.shape[0])).difference(usedRows) unusedCols = set(range(0, D.shape[1])).difference(usedCols) # in the event that the number of object centroids is # equal or greater than the number of input centroids # we need to check and see if some of these objects have # potentially disappeared if D.shape[0] &gt;= D.shape[1]: # loop over the unused row indexes for row in unusedRows: # grab the object ID for the corresponding row # index and increment the disappeared counter objectID = objectIDs[row] self.disappeared[objectID] += 1 # check to see if the number of consecutive # frames the object has been marked \"disappeared\" # for warrants deregistering the object if self.disappeared[objectID] &gt; self.maxDisappeared: self.deregister(objectID) # otherwise, if the number of input centroids is greater # than the number of existing object centroids we need to # register each new input centroid as a trackable object else: for col in unusedCols: self.register(inputCentroids[col]) # return the set of trackable objects return self.objects 需要源码扫描下方二维码回复。","link":"/2019/12/01/OpenCV实现目标跟踪/"},{"title":"在开始一个成功的机器学习项目之前要问的11个问题","text":"机器学习算法正在改变几乎每个产业。他们正提高生产力，促进销售并帮我们做出更明智的决定。很多组织也早已在利用机器学习的功能，或者放在其规划中作为一个值得追求的机会。 问题是ML很复杂，当开始一个ML项目的时候你很可能会问自己需要考虑什么。在你启动一个ML计划前，你应该花一些时间来计划。这将有助于最大程度减少潜在风险并最大化积极成果。 这十一个问题会考虑到你的策略，文化，资源和数据。这些问题将帮助你为ML项目构建战略路线图，并使实施更进一步。 STRATEGY1. 你组织的业务目标是什么？即使将ML作为实验开始，也应该始终有一个最终的业务目标，该目标实际上很重要，并且可能会受到ML的影响。机器学习项目的成功标准应该不是一定的准确度，而是业务指标。 例如，假设您正在零售领域，现在的主要重点是降低成本。一个特定的业务目标可能是在下半年末将仓库成本降低10％。实现此目标的一种方法可能是通过从ML算法获得的优化。 2. ML是应该降低成本还是增加收入？成功的机器学习项目可以降低成本或增加收入（或两者都有），您应该定义目标是什么。只有这样，ML才能对您的组织及其成长产生重大影响。 借助机器学习降低成本的示例包括虚拟助手和聊天机器人，预测性维护和库存优化。相反，实施了诸如价格优化，通过视频分析或推荐系统进行行为客户跟踪之类的解决方案来增加收入。 3. 衡量ML计划成功的清晰而现实的方法是什么?每个ML项目都是不同的，您需要定义一个成功指标，该指标对您的特定项目有意义，并且可以衡量。设置该指标后，您应该确保它被业务人员和数据科学家所接受。 机器学习计划的成功指标可能是工作时间，客户查询响应时间或准确性百分比。 4. 你的组织如何处理风险？与所有创新一样，由于多种因素，机器学习计划的结果可能不会如预期的那样出现。也许您发现需要10倍的数据才能做有意义的事情，或者数据根本不够好。这就是为什么您应该考虑ML计划失败的风险，并能够在项目未达到您的期望的情况下转到另一个项目。 CULTURE/RESOURCES5. 如何获得合适的人才？由于对ML的专业需求很高，因此在市场上很难获得。当您无法找到合适的人（或没有足够的人）来从事该项目时，完全或部分外包ML项目是一个不错的选择，尤其是在时机很重要且您希望早日实施解决方案时。 6. 你有没有对ML是什么有更清晰的理解？了解不同的用例，需要什么输入ML算法以及什么时候不能应用它们，可以帮助您在项目中做出正确的决定。您应该从概念上理解ML给图片带来的好处，例如，通过阅读机器学习博客和案例研究，参加会议或与专家交谈。 7. 是否保证信息的获取？当启动ML计划时，数据科学家需要轻松访问信息。他们将需要与关键人员（可能来自不同部门，例如IT部门）合作。那些人需要用某种商业知识来支持该项目，而内部官僚主义将是主要的制约因素。理想情况下，机器学习和数据科学应及时遍及整个组织。 “数据科学是新的统计方法”。 8. 是否将该项目计划成中期项目？机器学习计划的结果需要时间，您将无法在一夜之间证明自己的成功。这需要你的组织都把机器学习的主动性作为中期项目。在软件开发世界中，时间估计一直是一个挑战。在机器学习开发的情况下，这一挑战更加艰巨，因为流程本身具有更多的不确定性。有时，您可能会停滞不前，简单的突破就可以彻底扭转局面。也就是说，您应该注意，可能需要花费几个月的时间才能获得理想的结果。 DATA9. 你的组织收集了正确的数据吗？机器学习算法不是魔法。他们需要数据才能工作，并且只能与您输入的数据一样好。根据您的目标，有不同的方法和资源来收集正确的数据。无论如何，您拥有的输入数据越多，机器学习模型表现良好的机会就越大。如果您对数据的数量和质量有疑问，可以请数据科学家帮助您评估数据集，并在必要时找到获取第三方数据的最佳方法。 10. 你的组织是否以正确的格式收集数据？除了拥有正确数量和类型的数据外，您还应确保以正确的格式收集数据。想象一下，您已经为智能手机拍摄了数千张完美的照片（高分辨率和白色背景），以训练计算机视觉模型来检测图像中的智能手机。然后，您发现它不起作用，因为实际用例是检测到在各种照明/对比度/背景下拿着智能手机的人，而不是自己。您过去的数据收集工作几乎毫无价值，您需要重新开始。另外，您应该了解所收集的数据中是否存在偏差，因为ML算法将学习该偏差。 11. 是否考虑过人工标注数据？根据项目，机器学习解决方案可以基于监督算法。这些算法要求对收集的数据进行标记，即。人们需要为我们收集的每个示例指定预期的结果，以便算法可以从这些见解中学习。确保在项目预算中包括构建此类数据集的人员成本。","link":"/2019/11/01/开始一个机器学习项目需要知道的/"},{"title":"厦门","text":"个人觉得厦门没什么好玩的，被炒起来的网红城市。厦门的慢体现在生活中吧，如果去玩还是会差些。最毒的就是每天晚上都要去曾厝垵吃东西（芒果），吃到撑。","link":"/2019/10/12/厦门/"},{"title":"图像识别工程步骤","text":"数据准备 拍摄照片或下载数据集 图像标注 图像数据处理（图像合成、图像裁剪、图像改变大小） 检查标注文件的正确性 划分训练集、验证集 搭建神经网络 模型训练（读取数据的多线程生成器、多GPU并行训练、保存模型） 模型评估（加载模型、使用业务对应的评估指标在验证集上计算模型效果） 模型测试（使用实际场景中的图像数据测试模型效果，单张图片检查、实时视频检测） 模型部署（开发web服务端接口、前端界面） 闭环优化 根据业务使用场景在第2步优化 根据模型在实际场景的使用效果在3-6步间循环迭代。如果新加入的数据提高了模型在验证集上的效果，则替换模型部署使用的权重文件。 如果替换后的新权重文件在实际场景中的使用效果不好，则使用版本控制工具回退到上一个版本的权重文件。","link":"/2019/09/30/图像识别工程大概步骤/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/08/30/hello-world/"},{"title":"hello","text":"aa今天是0831","link":"/2019/08/28/hello/"},{"title":"My New Post","text":"这是第二个博客 2019年8月29","link":"/2019/08/28/My-New-Post/"},{"title":"A SImple Post'","text":"Here is simaple markdown","link":"/2019/08/28/Post/"},{"title":"test_my_site","text":"","link":"/2019/08/28/test-my-site/"}],"tags":[{"name":"Travel","slug":"Travel","link":"/tags/Travel/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"}],"categories":[]}